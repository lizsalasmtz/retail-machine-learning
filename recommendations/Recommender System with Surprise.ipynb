{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# if using Anaconda, install scikit-surprise to your conda environment using:\n",
    "# conda install -c conda-forge scikit-surprise (pip install sckit-surprise yields errors...)\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import NormalPredictor\n",
    "from surprise import KNNBasic\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNWithZScore\n",
    "from surprise import KNNBaseline\n",
    "from surprise import SVD\n",
    "from surprise import BaselineOnly\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import dump\n",
    "\n",
    "\n",
    "# define current working folder\n",
    "# curr_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# user.columns = ['userID', 'Location', 'Age']\n",
    "# rating = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "# rating.columns = ['userID', 'ISBN', 'bookRating']\n",
    "\n",
    "# import tables from Instacart Kaggle dataset\n",
    "aisles = pd.read_csv(os.path.join(curr_dir, 'aisles.csv'))\n",
    "departments = pd.read_csv(os.path.join(curr_dir, 'departments.csv'))\n",
    "order_products_prior = pd.read_csv(os.path.join(curr_dir, 'order_products__prior.csv'))\n",
    "order_products_train = pd.read_csv(os.path.join(curr_dir, 'order_products__train.csv'))\n",
    "orders = pd.read_csv(os.path.join(curr_dir, 'orders.csv'))\n",
    "products = pd.read_csv(os.path.join(curr_dir, 'products.csv'))\n",
    "\n",
    "# stack the previous train/\"prior\" datasets used for training/test\n",
    "# we want to redo the train/test groups and not rely on the Kaggle \n",
    "# dataset creator's groups since we're going to drop data\n",
    "order_products = order_products_train\n",
    "order_products = order_products.append(order_products_prior)\n",
    "\n",
    "# print('\\nORDER_PRODUCT_DUPS')\n",
    "# print(order_products_dups)\n",
    "\n",
    "del order_products_prior\n",
    "del order_products_train\n",
    "\n",
    "# merge the tables together because there are no duplicates on primary keys \n",
    "all_merged_data = pd.merge(order_products, products, how='left', on=['product_id', 'product_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, orders, how='left', on=['order_id', 'order_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, aisles, how='left', on=['aisle_id', 'aisle_id'])\n",
    "all_merged_data = pd.merge(all_merged_data, departments, how='left', on=['department_id', 'department_id'])\n",
    "\n",
    "del aisles\n",
    "del departments\n",
    "del orders\n",
    "del products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "41.28% of product codes have 40 or fewer sales accounting for 1.01% of all sales\n54.97% of product codes have 80 or fewer sales accounting for 2.18% of all sales\n62.22% of product codes have 120 or fewer sales accounting for 3.23% of all sales\n67.15% of product codes have 160 or fewer sales accounting for 4.24% of all sales\n70.53% of product codes have 200 or fewer sales accounting for 5.14% of all sales\n73.19% of product codes have 240 or fewer sales accounting for 6.0% of all sales\n75.42% of product codes have 280 or fewer sales accounting for 6.85% of all sales\n77.33% of product codes have 320 or fewer sales accounting for 7.69% of all sales\n78.92% of product codes have 360 or fewer sales accounting for 8.48% of all sales\n80.2% of product codes have 400 or fewer sales accounting for 9.19% of all sales\n81.35% of product codes have 440 or fewer sales accounting for 9.91% of all sales\n82.32% of product codes have 480 or fewer sales accounting for 10.56% of all sales\n83.24% of product codes have 520 or fewer sales accounting for 11.24% of all sales\n84.06% of product codes have 560 or fewer sales accounting for 11.88% of all sales\n84.82% of product codes have 600 or fewer sales accounting for 12.53% of all sales\n85.45% of product codes have 640 or fewer sales accounting for 13.1% of all sales\n86.09% of product codes have 680 or fewer sales accounting for 13.73% of all sales\n86.63% of product codes have 720 or fewer sales accounting for 14.28% of all sales\n87.11% of product codes have 760 or fewer sales accounting for 14.8% of all sales\n87.62% of product codes have 800 or fewer sales accounting for 15.39% of all sales\n"
     ]
    }
   ],
   "source": [
    "# CUT DOWN ON PRODUCTS\n",
    "# Need to reduce the number of products in the user/products matrix for recommendations (123k users x 49k products = ~110GB of RAM required for float distance calculations)\n",
    "\n",
    "# get the products and the number of orders they were sold in\n",
    "product_dist = all_merged_data.groupby(['product_id','product_name']).size().sort_values(ascending=False).reset_index()\n",
    "product_dist.columns = ['product_id','product_name','count']\n",
    "\n",
    "# loop through the count of products by order they were sold in to see\n",
    "# information about products with very low sales (like % of volume) \n",
    "for i in range(1,21):\n",
    "    threshold = 40 * i\n",
    "\n",
    "    pct_of_prod_ids_under_threshold = 100 * product_dist['product_id'].loc[product_dist['count'] <= threshold].count() / product_dist['product_id'].count()\n",
    "    \n",
    "    pct_of_prod_volume_sold_under_threshold = 100 * product_dist['count'].loc[product_dist['count'] <= threshold].sum() / product_dist['count'].sum()\n",
    "\n",
    "    print(str(round(pct_of_prod_ids_under_threshold,2)) + '% of product codes have ' + str(threshold) + \\\n",
    "            ' or fewer sales accounting for ' + str(round(pct_of_prod_volume_sold_under_threshold,2)) + '% of all sales')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0% of users have 1 or fewer orders, or 0.0% of all orders or 0.0% of all product volume.\n0.0% of users have 2 or fewer orders, or 0.0% of all orders or 0.0% of all product volume.\n4.21% of users have 3 or fewer orders, or 0.78% of all orders or 0.74% of all product volume.\n15.1% of users have 4 or fewer orders, or 3.46% of all orders or 3.29% of all product volume.\n23.96% of users have 5 or fewer orders, or 6.19% of all orders or 5.92% of all product volume.\n31.39% of users have 6 or fewer orders, or 8.94% of all orders or 8.59% of all product volume.\n37.79% of users have 7 or fewer orders, or 11.7% of all orders or 11.28% of all product volume.\n43.16% of users have 8 or fewer orders, or 14.35% of all orders or 13.88% of all product volume.\n47.9% of users have 9 or fewer orders, or 16.98% of all orders or 16.48% of all product volume.\n52.1% of users have 10 or fewer orders, or 19.56% of all orders or 19.05% of all product volume.\n55.69% of users have 11 or fewer orders, or 22.0% of all orders or 21.44% of all product volume.\n58.98% of users have 12 or fewer orders, or 24.43% of all orders or 23.85% of all product volume.\n61.85% of users have 13 or fewer orders, or 26.73% of all orders or 26.17% of all product volume.\n64.5% of users have 14 or fewer orders, or 29.02% of all orders or 28.43% of all product volume.\n66.91% of users have 15 or fewer orders, or 31.24% of all orders or 30.69% of all product volume.\n69.08% of users have 16 or fewer orders, or 33.39% of all orders or 32.81% of all product volume.\n71.01% of users have 17 or fewer orders, or 35.41% of all orders or 34.82% of all product volume.\n72.81% of users have 18 or fewer orders, or 37.4% of all orders or 36.83% of all product volume.\n74.4% of users have 19 or fewer orders, or 39.27% of all orders or 38.72% of all product volume.\n"
     ]
    }
   ],
   "source": [
    "# CUT DOWN ON USERS\n",
    "# Need to reduce the number of users in the user/products matrix for recommendations (123k users x 49k products = ~110GB of RAM required for float distance calculations)\n",
    "\n",
    "# get number of unique orders by user\n",
    "user_order_dist = all_merged_data.groupby('user_id')['order_id'].nunique().reset_index()\n",
    "user_order_dist.columns = ['user_id','order_count']\n",
    "user_order_dist = user_order_dist.sort_values(by='order_count',ascending=False)\n",
    "user_order_dist\n",
    "\n",
    "# get number of product rows (products are counted for each order they appear in) - this is the number of rows removed from the data\n",
    "user_order_prod_count_dist = all_merged_data.groupby('user_id')['product_id'].count().reset_index()\n",
    "user_order_prod_count_dist.columns = ['user_id','product_by_order_count']\n",
    "user_order_prod_count_dist = user_order_prod_count_dist.sort_values(by='product_by_order_count',ascending=False)\n",
    "user_order_prod_count_dist\n",
    "\n",
    "# merge the order count with the total rows\n",
    "user_dist = pd.merge(user_order_dist,user_order_prod_count_dist, how='inner', on='user_id').reset_index()\n",
    "\n",
    "# loop through order counts by user and show product sold (count them for each order)\n",
    "# to see where to cut the data off\n",
    "for i in range(1,20):\n",
    "\n",
    "    threshold = i\n",
    "\n",
    "    pct_of_user_ids_under_threshold = 100 * user_dist['user_id'].loc[user_dist['order_count'] <= threshold].count() / user_dist['user_id'].count()\n",
    "    pct_of_user_volume_sold_under_threshold = 100 * user_dist['order_count'].loc[user_dist['order_count'] <= threshold].sum() / user_dist['order_count'].sum()\n",
    "\n",
    "    pct_of_sales_volume_sold_under_threshold = 100 * user_dist['product_by_order_count'].loc[user_dist['order_count'] <= threshold].sum() / user_order_prod_count_dist['product_by_order_count'].sum()\n",
    "\n",
    "    print(str(round(pct_of_user_ids_under_threshold,2)) + '% of users have ' + str(threshold) + \\\n",
    "            ' or fewer orders, or ' + str(round(pct_of_user_volume_sold_under_threshold,2)) + \\\n",
    "            '% of all orders' + ' or ' + str(round(pct_of_sales_volume_sold_under_threshold,2)) +'% of all product volume.')\n",
    "\n",
    "del user_order_prod_count_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nTOP PRODUCTS (BY ORDERS CONTAINING PRODUCT)\n   product_id            product_name   count\n0       24852                  Banana  491291\n1       13176  Bag of Organic Bananas  394930\n2       21137    Organic Strawberries  275577\n3       21903    Organic Baby Spinach  251705\n4       47209    Organic Hass Avocado  220877\n5       47766         Organic Avocado  184224\n6       47626             Large Lemon  160792\n7       16797            Strawberries  149445\n8       26209                   Limes  146660\n9       27845      Organic Whole Milk  142813\n\n\nTOP USERS (BY ORDERS CONTAINING PRODUCT)\n   index  user_id  order_count  product_by_order_count\n0      0   112841          100                     763\n1      1   174555          100                     874\n2      2    73676          100                     661\n3      3   176469          100                     622\n4      4    90584          100                     351\n5      5   190487          100                     795\n6      6    57367          100                     539\n7      7    81549          100                     444\n8      8    77880          100                     803\n9      9   190456          100                     688\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nTOP PRODUCTS (BY ORDERS CONTAINING PRODUCT)')\n",
    "print(product_dist.head(10))\n",
    "\n",
    "print('\\n\\nTOP USERS (BY ORDERS CONTAINING PRODUCT)')\n",
    "print(user_dist.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original dataframe shape:\t\t\t(33819106, 15)\n",
      "Deleted users and products dataframe shape:\t (21455288, 4)\n",
      "Grouped model dataframe shape:\t\t\t (6709570, 5)\n"
     ]
    }
   ],
   "source": [
    "# DROP LOW VALUE RECORDS TO REDUCE 'DIMENSIONALITY'\n",
    "# AND CONVERT THE REORDERING TO A 5-POINT SCALE SYSTEM BASED ON NUMBER OF REORDERS BY A USER \n",
    "\n",
    "print('Original dataframe shape:\\t\\t\\t{}'.format(all_merged_data.shape))\n",
    "\n",
    "# choose product sales volume threshold for inclusion in model and drop low-sale-count \n",
    "# products from all_merged data (i.e. not enough purchases to make good recommendations)\n",
    "product_threshold = 400\n",
    "product_dist.drop(product_dist['product_id'].loc[product_dist['count'] <= product_threshold].index, inplace=True)\n",
    "all_merged_data_reduced = pd.merge(all_merged_data, product_dist['product_id'], how='inner', on='product_id')\n",
    "\n",
    "# choose product sales volume threshold for inclusion in model and drop low-sale-count \n",
    "# users from all_merged data (likely not enough purchases to make good recommendations)\n",
    "user_threshold = 15\n",
    "user_dist.drop(user_dist['user_id'].loc[user_dist['order_count'] <= user_threshold].index, inplace=True)\n",
    "all_merged_data_reduced = pd.merge(all_merged_data_reduced, user_dist['user_id'], how='inner', on='user_id')\n",
    "\n",
    "# drop columns not necessary to recommendation engine\n",
    "all_merged_data_reduced.drop(['order_id','add_to_cart_order','aisle_id','department_id','eval_set','order_number','order_dow','order_hour_of_day','days_since_prior_order','aisle','department'], axis=1, inplace=True)\n",
    "\n",
    "# show the raw reduction in shape\n",
    "print('Deleted users and products dataframe shape:\\t {}'.format(all_merged_data_reduced.shape))\n",
    "\n",
    "# group the resulting dataset on user_id and product_id \n",
    "# map the number of reorders by user/product to a 5-point scale \n",
    "model_data = all_merged_data_reduced.reset_index()\n",
    "\n",
    "model_data = model_data.groupby(['user_id','product_id','product_name'])['reordered'].sum().reset_index()\n",
    "model_data.columns = ['user_id','product_id','product_name','reordered_count']\n",
    "\n",
    "def get_rating(row):\n",
    "    if row['reordered_count'] == 0:\n",
    "        return 1\n",
    "    elif row['reordered_count'] == 1:\n",
    "        return  2\n",
    "    elif row['reordered_count'] <= 4:\n",
    "        return 3\n",
    "    elif row['reordered_count'] <= 9:\n",
    "        return 4\n",
    "    elif row['reordered_count'] <= 99:\n",
    "        return 5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "model_data['rating'] = model_data.apply (lambda row: get_rating(row), axis=1)\n",
    "model_data['rating_obj'] = model_data['rating'].astype(object) # for pivot table category\n",
    "\n",
    "model_data = model_data.drop(['reordered_count'], axis=1)\n",
    "\n",
    "print('Grouped model dataframe shape:\\t\\t\\t {}'.format(model_data.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             rating                   \n",
       "rating_obj                                        1    2    3    4   5\n",
       "product_name                                                          \n",
       "#2 Coffee Filters                               246   71   41    8   1\n",
       "0% Fat Blueberry Greek Yogurt                    78   18   19   14   8\n",
       "0% Fat Free Organic Milk                        183   73  118   66  69\n",
       "0% Fat Organic Greek Vanilla Yogurt             150   48   52   38  26\n",
       "0% Fat Superfruits Greek Yogurt                  93   26   28   13  10\n",
       "...                                             ...  ...  ...  ...  ..\n",
       "of Hanover 100 Calorie Pretzels Mini            301  164  160   64  40\n",
       "smartwater® Electrolyte Enhanced Water          328  114  146   81  78\n",
       "vitaminwater® XXX Acai Blueberry Pomegranate    223   65   74   34  23\n",
       "with Crispy Almonds Cereal                      264  123  152  100  76\n",
       "with Olive Oil Mayonnaise                       162   38   21    1   0\n",
       "\n",
       "[9837 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"5\" halign=\"left\">rating</th>\n    </tr>\n    <tr>\n      <th>rating_obj</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n    </tr>\n    <tr>\n      <th>product_name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>#2 Coffee Filters</th>\n      <td>246</td>\n      <td>71</td>\n      <td>41</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>0% Fat Blueberry Greek Yogurt</th>\n      <td>78</td>\n      <td>18</td>\n      <td>19</td>\n      <td>14</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>0% Fat Free Organic Milk</th>\n      <td>183</td>\n      <td>73</td>\n      <td>118</td>\n      <td>66</td>\n      <td>69</td>\n    </tr>\n    <tr>\n      <th>0% Fat Organic Greek Vanilla Yogurt</th>\n      <td>150</td>\n      <td>48</td>\n      <td>52</td>\n      <td>38</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>0% Fat Superfruits Greek Yogurt</th>\n      <td>93</td>\n      <td>26</td>\n      <td>28</td>\n      <td>13</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>of Hanover 100 Calorie Pretzels Mini</th>\n      <td>301</td>\n      <td>164</td>\n      <td>160</td>\n      <td>64</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>smartwater® Electrolyte Enhanced Water</th>\n      <td>328</td>\n      <td>114</td>\n      <td>146</td>\n      <td>81</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>vitaminwater® XXX Acai Blueberry Pomegranate</th>\n      <td>223</td>\n      <td>65</td>\n      <td>74</td>\n      <td>34</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>with Crispy Almonds Cereal</th>\n      <td>264</td>\n      <td>123</td>\n      <td>152</td>\n      <td>100</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>with Olive Oil Mayonnaise</th>\n      <td>162</td>\n      <td>38</td>\n      <td>21</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9837 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "pivot = pd.pivot_table(model_data, index=['product_name'], columns=['rating_obj'], values=['rating'], aggfunc='count', fill_value=0)\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         user_id  product_id  rating\n",
       "0              7         274       1\n",
       "1              7         519       2\n",
       "2              7        4920       4\n",
       "3              7        4945       3\n",
       "4              7        6361       3\n",
       "...          ...         ...     ...\n",
       "6709565   206208       48017       1\n",
       "6709566   206208       48364       1\n",
       "6709567   206208       48865       1\n",
       "6709568   206208       49247       1\n",
       "6709569   206208       49621       1\n",
       "\n",
       "[6709570 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>product_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>274</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>519</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n      <td>4920</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>4945</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>6361</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6709565</th>\n      <td>206208</td>\n      <td>48017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6709566</th>\n      <td>206208</td>\n      <td>48364</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6709567</th>\n      <td>206208</td>\n      <td>48865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6709568</th>\n      <td>206208</td>\n      <td>49247</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6709569</th>\n      <td>206208</td>\n      <td>49621</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>6709570 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model_data.drop(['product_name','rating_obj'], axis=1, inplace=True)\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6709570 entries, 0 to 6709569\nData columns (total 3 columns):\n #   Column      Dtype\n---  ------      -----\n 0   user_id     int64\n 1   product_id  int64\n 2   rating      int64\ndtypes: int64(3)\nmemory usage: 153.6 MB\n"
     ]
    }
   ],
   "source": [
    "model_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up once you've sufficiently reduced the dimensionality above\n",
    "del product_dist\n",
    "del user_dist\n",
    "del all_merged_data\n",
    "del all_merged_data_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset shape: (6709570, 3)\n-Dataset examples-\n         user_id  product_id  scale\n0              7         274      1\n500000     15553       14992      4\n1000000    30947       22935      5\n1500000    45991       37436      4\n2000000    61399       27336      5\n2500000    76460         160      1\n3000000    91631       41375      1\n3500000   106863       14371      5\n4000000   122166         890      4\n4500000   137916       17122      5\n5000000   153163        8048      1\n5500000   168950        7901      5\n6000000   184461       26709      3\n6500000   199737       42768      3\n"
     ]
    }
   ],
   "source": [
    "print('Dataset shape: {}'.format(model_data.shape))\n",
    "print('-Dataset examples-')\n",
    "print(model_data.iloc[::500000, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the merged data as a csv (so that it can be imported later without needing to re-merge)\n",
    "model_data.to_csv(os.path.join(curr_dir,'model_data.csv'), sep=',', encoding='utf-8', index=False)\n",
    "\n",
    "# load the merged data as a csv (so that it can be imported later without needing to re-merge)\n",
    "# model_data = pd.read_csv(os.path.join(curr_dir,'model_data.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data into Surprise using Reader\n",
    "reorder = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(model_data[['user_id', 'product_id', 'rating']], reorder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the trainset and testset\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimating biases using sgd...\n",
      "RMSE: 1.1388\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.1388271656447095"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# use the SVD algorithm\n",
    "algo = SVD(n_epochs = 20, n_factors = 50, verbose = True)\n",
    "\n",
    "# train the model on trainset and validate predictions for known ratings on the testset\n",
    "predictions = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Prediction(uid=133467, iid=21137, r_ui=1.0, est=1.9286465477468584, details={'was_impossible': False}),\n",
       " Prediction(uid=178888, iid=45007, r_ui=2.0, est=2.4434226423476555, details={'was_impossible': False}),\n",
       " Prediction(uid=79046, iid=44625, r_ui=5.0, est=3.0983893867160845, details={'was_impossible': False}),\n",
       " Prediction(uid=104042, iid=37570, r_ui=1.0, est=1.5305284267455972, details={'was_impossible': False}),\n",
       " Prediction(uid=176283, iid=16797, r_ui=3.0, est=2.4323762463254037, details={'was_impossible': False}),\n",
       " Prediction(uid=2476, iid=11073, r_ui=3.0, est=2.715419083321947, details={'was_impossible': False}),\n",
       " Prediction(uid=193275, iid=39928, r_ui=3.0, est=2.039766100330349, details={'was_impossible': False}),\n",
       " Prediction(uid=131792, iid=20632, r_ui=5.0, est=2.405726807603433, details={'was_impossible': False}),\n",
       " Prediction(uid=203160, iid=4472, r_ui=1.0, est=2.395816870653601, details={'was_impossible': False}),\n",
       " Prediction(uid=49307, iid=34448, r_ui=2.0, est=2.7535834444953857, details={'was_impossible': False})]"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "# show the first n predictions for the trainset (actual and estimated rating)\n",
    "predictions[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1e186aa9be0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# define the trainset to be the entire dataset \n",
    "# (to generate predictions for ALL the products a user has not purchased)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# define the model\n",
    "algo = SVD(n_epochs = 20, n_factors = 50, verbose = True)\n",
    "\n",
    "# train the model\n",
    "algo.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7\n",
      "records processed:\t0\n",
      "15\n",
      "17\n",
      "21\n",
      "24\n",
      "27\n",
      "28\n",
      "29\n",
      "31\n",
      "36\n",
      "37\n",
      "42\n",
      "46\n",
      "50\n",
      "52\n",
      "54\n",
      "63\n",
      "67\n",
      "71\n",
      "75\n",
      "82\n",
      "86\n",
      "87\n",
      "89\n",
      "90\n",
      "91\n",
      "96\n",
      "99\n",
      "101\n",
      "103\n",
      "110\n",
      "112\n",
      "118\n",
      "122\n",
      "127\n",
      "132\n",
      "133\n",
      "138\n",
      "140\n",
      "142\n",
      "146\n",
      "150\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "161\n",
      "168\n",
      "173\n",
      "174\n",
      "175\n",
      "182\n",
      "187\n",
      "190\n",
      "195\n",
      "197\n",
      "204\n",
      "206\n",
      "208\n",
      "209\n",
      "210\n",
      "214\n",
      "216\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "226\n",
      "227\n",
      "229\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "242\n",
      "243\n",
      "248\n",
      "256\n",
      "257\n",
      "258\n",
      "260\n",
      "262\n",
      "264\n",
      "270\n",
      "271\n",
      "273\n",
      "278\n",
      "280\n",
      "281\n",
      "284\n",
      "289\n",
      "290\n",
      "294\n",
      "295\n",
      "300\n",
      "301\n",
      "304\n",
      "306\n"
     ]
    }
   ],
   "source": [
    "# loop through all users and all product combinations in model_data and add recommendations to a dataframe\n",
    "# PRODUCTS WITH SUFFICIENT SALES WILL BE RECOMMENDED TO USERS WITH SUFFICIENT HISTORY\n",
    "\n",
    "# all distinct users\n",
    "all_users = model_data['user_id'].unique()\n",
    "all_users = pd.DataFrame(all_users, columns=['user_id']).reset_index()\n",
    "\n",
    "# all distinct products\n",
    "model_products = model_data['product_id'].unique()\n",
    "model_products = pd.DataFrame(model_products, columns=['product_id'])\n",
    "model_products = pd.merge(model_products, products, how='inner', on='product_id')\n",
    "\n",
    "recommendations = pd.DataFrame(columns=['user_id','product_id','product_name','model_rating'])\n",
    "\n",
    "for u in all_users.itertuples():\n",
    "    if u[0] % 1000 == 0:\n",
    "        print('records processed:\\t' + str(u[0]))\n",
    "\n",
    "    # for selected user, rate all products they haven't bought before\n",
    "    user_ratings = []\n",
    "    for i in model_products.itertuples():\n",
    "        dict = {'user_id':u[2],\n",
    "                    'product_id':i[1],\n",
    "                    'product_name':i[2],\n",
    "                    'aisle_id':i[3],\n",
    "                    'department_id':i[4],\n",
    "                    'model_rating': algo.predict(u[2], i[1], verbose=False)[3] \n",
    "                }\n",
    "        user_ratings.append(dict)\n",
    "\n",
    "    df = pd.DataFrame(user_ratings).sort_values(by='model_rating',ascending=False).reset_index()\n",
    "    df = df.groupby('aisle_id').first().sort_values(by='model_rating',ascending=False).reset_index()\n",
    "    df = df.drop(['aisle_id','index','department_id'],axis=1)\n",
    "    df = df.iloc[0:10,]\n",
    "    recommendations = recommendations.append(df)\n",
    "\n",
    "recommendations.to_csv(os.path.join(curr_dir,'recommendations.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nInt64Index: 200 entries, 0 to 9\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   user_id       200 non-null    object \n 1   product_id    200 non-null    object \n 2   product_name  200 non-null    object \n 3   model_rating  200 non-null    float64\ndtypes: float64(1), object(3)\nmemory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "recommendations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}